{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22aa85e0-e0af-40f4-be5e-66fe1b60f851",
   "metadata": {},
   "source": [
    "# ML Experiment with FPKM expression data from selected modENCODE libraries\n",
    "---\n",
    "\n",
    "Given the ___median expression level___ of a gene across classical age groups, can you determine the WormCat category using Machine Learning?\n",
    "\n",
    "__Notes__\n",
    "\n",
    "<table align='left'>\n",
    "    <tr><th>Select</th><th>Count</th></tr>\n",
    "    <tr><td>All Rows</td><td align='right'>6,648,770</td></tr>\n",
    "    <tr><td>Median Value Rows</td><td align='right'>1,791,982</td></tr>\n",
    "    <tr><td>Classical Life Stages (CLS)</td><td align='right'>330,096</td></tr>\n",
    "    <tr><td>CLS + Duaer</td><td align='right'>377,254</td></tr>\n",
    "     <tr><td>Merge WormCat Drop NA's</td><td align='right'>247,056</td></tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb750b99-4148-4fcb-89aa-ea850447cf76",
   "metadata": {},
   "source": [
    "## Create Raw Data for Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49531f0f-985e-44e6-beb7-314fb2970985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import platform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f51be9a-44f9-4d4e-aef6-dd16a73c6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Graph data from Wormbase\n",
    "\n",
    "INPUT_DATA='./input_data'\n",
    "expr_graph=f'{INPUT_DATA}/c_elegans.PRJNA13758.WS287.expr_graph.csv'\n",
    "clean_str = lambda x: x.replace('\"','').strip()\n",
    "clean_float = lambda x: float(x.replace('\"','').strip())\n",
    "\n",
    "columns = [\"Gene\", \"Gene_name\", \"Life_stage\", \"Library\", \"Protocol\", \"FPKM_value\"]\n",
    "expr_graph_df = pd.read_csv(expr_graph,low_memory=False, header=None, names=columns,\n",
    "                            skiprows=1,\n",
    "                            converters={'Gene_name':clean_str, 'Life_stage':clean_str, \n",
    "                                        'Library':clean_str, 'Protocol':clean_str,\n",
    "                                        'FPKM_value':clean_float})\n",
    "print(f\"{len(expr_graph_df):,} rows of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dda5f5-d194-438d-8930-4886b1d391e6",
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "expr_graph_median_protocol_df = expr_graph_df[expr_graph_df['Library']=='Median']\n",
=======
    "expr_graph_median_protocol_df = expr_graph_df[expr_graph_df['Protocol']=='Median']\n",
>>>>>>> 4f9883b7b192f8a4e75b4a862bbde3e23f50b29a
    "print(f\"{len(expr_graph_median_protocol_df):,} rows of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3aa2b-a3c5-496f-973f-15468d7be742",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_life_stage_df = expr_graph_median_protocol_df.query(\"Life_stage in ['EE','LE','L1','L2','L3','L4','YA', 'Dauer']\")\n",
    "print(f\"{len(classical_life_stage_df):,} rows of data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ccbf5-cfed-4028-8faf-26691cbbc630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#classical_life_stage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99e27e3-32b9-4d0d-a513-aff8bf2650a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe with approriate dimensions for ML\n",
    "\n",
    "classical_life_stage_dict = {}\n",
    "empty_entry = {'EE':None,'LE':None,'L1':None,'L2':None,'L3':None,'L4':None,'YA':None, 'Dauer':None}\n",
    "\n",
    "for index, row in classical_life_stage_df.iterrows():\n",
    "    if row['Gene'] not in classical_life_stage_dict:\n",
    "        classical_life_stage_dict[row['Gene']] = empty_entry.copy()\n",
    "        \n",
    "    classical_life_stage_dict[row['Gene']][row['Life_stage']]=row['FPKM_value']\n",
    "                                           \n",
    "classical_life_stage_df2 = pd.DataFrame.from_dict(classical_life_stage_dict, orient='index')\n",
    "classical_life_stage_df2 = classical_life_stage_df2.reset_index()\n",
    "classical_life_stage_df2 = classical_life_stage_df2.rename(columns={'index':'wormbase_id'})\n",
    "#classical_life_stage_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "25ce5d94-ab0f-44c4-80b5-35da50e1ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_life_stage_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
>>>>>>> 4f9883b7b192f8a4e75b4a862bbde3e23f50b29a
   "id": "c33a86ef-9169-4281-869b-ecdb1ffbdf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the wormbase category list\n",
    "# Setup. to be queried\n",
    "wormcat_df = pd.read_csv('./input_data/whole_genome_v2_nov-11-2021.csv') \n",
    "wormcat_df = wormcat_df.rename(columns={'Sequence ID':'sequence_id','Wormbase ID':'wormbase_id','Category 1':'category_1','Category 2':'category_2','Category 3':'category_3'})\n",
    "wormcat_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bacf18-e692-4221-804b-5de207568b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_life_stage_df3 = pd.merge(classical_life_stage_df2, wormcat_df, on='wormbase_id', how='left')\n",
    "print(f\"{len(classical_life_stage_df3):,} rows of data {len(classical_life_stage_df3)*8:,} Data Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251a485-d6d2-4884-8ac9-110fb00e43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_life_stage_df4 = classical_life_stage_df3.dropna()\n",
    "print(f\"{len(classical_life_stage_df4):,} rows of data {len(classical_life_stage_df4)*8:,} Data Points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865422d-50ab-4cc5-bf53-5748b2015e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_columns=['category_2','category_3','sequence_id']\n",
    "automated_description='Automated Description'\n",
    "\n",
    "if automated_description in classical_life_stage_df4.columns:\n",
    "    delete_columns +=[automated_description]\n",
    "    \n",
    "classical_life_stage_df4 = classical_life_stage_df4.drop(columns=delete_columns)\n",
    "classical_life_stage_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c75f64-1c3c-45a3-8b7d-3446ee119ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir ='./output_data/output_classical_life_stage'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "    \n",
    "classical_life_stage_df4.to_csv(f'{out_dir}/classical_life_stage.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81bbe8-c874-4593-aba5-f415256b6b46",
   "metadata": {},
   "source": [
    "## Preprocess and Evaluate Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e18900-a75b-47a5-b798-1925ec37095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from jupyter_utilities import formatted_elapsed_time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0af225-07a2-46dc-8bec-e137194d629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir ='./output_data/output_classical_life_stage'\n",
    "classical_life_stages_df = pd.read_csv(f'{out_dir}/classical_life_stage.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1ea7b-b116-41b9-9135-89fd1159261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pair plot\n",
    "start = time.time()\n",
    "%matplotlib inline\n",
    "sns.pairplot(classical_life_stages_df,hue='category_1')\n",
    "plt.savefig(out_dir + \"/pair_plot_classical_life_stages.png\")\n",
    "\n",
    "print(f\"Elapsed {formatted_elapsed_time(start)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db3d3f-c262-484a-abff-3fc724853772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the continuous data\n",
    "# THE RESULTS DOE NOT SHOW FAVORIBLE GAINS FROM SCALER\n",
    "start = time.time()\n",
    "#scaler = MinMaxScaler() DOES NOT IMPROVE THE RESULTS\n",
    "#continuous = ['EE','LE','L1','L2','L3','L4','YA','Dauer']\n",
    "#classical_life_stage_df4[continuous] = scaler.fit_transform(classical_life_stage_df4[continuous])\n",
    "print(formatted_elapsed_time(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad11fc52-3a3f-4049-80f8-7ff8d3218999",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = classical_life_stage_df4['category_1']\n",
    "features = classical_life_stage_df4.drop('category_1', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features,\n",
    "                                                    target,\n",
    "                                                    shuffle=True,\n",
    "                                                    test_size=0.15,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Balance the training dataset\n",
    "#X_train, y_train = balance_data(X_train, y_train)\n",
    "#X_train, y_train = balance_data(X_train, y_train,up_sample=False)\n",
    "\n",
    "\n",
    "# Add the Target back before saving\n",
    "X_train = X_train.assign(category_1=y_train.values)\n",
    "X_test = X_test.assign(category_1=y_test.values)\n",
    "\n",
    "\n",
    "# Show the results of the split\n",
    "print(f\"Training set has {X_train.shape[0]:,d} samples.\")\n",
    "print(f\"Testing set has {X_test.shape[0]:,d} samples.\")\n",
    "print(f\"Total set has {X_test.shape[0] + X_train.shape[0]:,d} samples.\")\n",
    "print(f\"Total Features {X_test.shape[1]-1:,d}.\")\n",
    "\n",
    "X_train.to_csv(out_dir + '/classical_life_stages-train.csv', index=None, header=True)\n",
    "X_test.to_csv(out_dir + '/classical_life_stages-test.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b061a0-293d-4e8a-a021-c31f3e469947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4f831d-fee2-4262-b2fc-6d8be88973dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda available\")\n",
    "else:\n",
    "    print(\"cpu only no cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7f7f2-cb2f-40d5-802f-cd28f0f51d96",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1cf87a-5f95-4e6c-80a7-6c077649268c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import platform\n",
    "import math\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils_data\n",
    "from sklearn.model_selection import KFold\n",
    "from jupyter_utilities import formatted_elapsed_time\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "\n",
    "BASE_DIR='./output_data/output_classical_life_stage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3bfafa-1b7c-47d4-b7dc-6d65f034a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiClassClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiClassClassifier, self).__init__()\n",
    "        self.hidden_1 = nn.Linear(8, 50)  # Input layer -> Hidden layer\n",
    "        #self.hidden_2 = nn.Linear(50, 25) # SET_DATA_SET\n",
    "        self.output = nn.Linear(50, 34) # ACTUAL DATA_SET\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden_1(x))\n",
    "        #x = torch.relu(self.hidden_2(x))\n",
    "        #x = torch.softmax(self.output(x), dim=1)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "def get_life_stages_data(base_dir=BASE_DIR, data_type='train'):\n",
    "    file_name_prefix='classical_life_stages'\n",
    "    data_set = pd.read_csv(f\"{base_dir}/{file_name_prefix}-{data_type}.csv\")\n",
    "    print(f\"{file_name_prefix}-{data_type}.shape {data_set.shape}\")\n",
    "    #print(f\"{data_set.columns}\")                   \n",
    "    print(f\"{data_set.head(1)}\")                   \n",
    "    print(\"=\"*60)\n",
    "    target_label = 'category_1'\n",
    "    x_train = data_set.drop(target_label, axis=1)\n",
    "    x_train = x_train.drop('wormbase_id', axis=1)\n",
    "    \n",
    "    y_train = data_set[target_label]\n",
    "    y_train = y_train.to_frame()\n",
    "\n",
    "    ##################\n",
    "    # Instantiate a RandomOverSampler object\n",
    "    #oversampler = RandomOverSampler()\n",
    "\n",
    "    # Fit and apply the oversampling on the training data\n",
    "    #x_train, y_train = oversampler.fit_resample(x_train, y_train)\n",
    "\n",
    "    ##################\n",
    "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(y_train)\n",
    "    y_train = one_hot_encoder.transform(y_train)\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    x_train = torch.from_numpy(x_train.values).float()\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    #y_train= y_train.unsqueeze(1)\n",
    "\n",
    "    return x_train, y_train, one_hot_encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd784479-d90f-4bb5-8c34-be3517c142aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset for learning\n",
    "class LearningDataset(utils_data.Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = x_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_item = self.x_data[index]\n",
    "        y_item = self.y_data[index]\n",
    "        return x_item, y_item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "\n",
    "def get_dataloaders(x_train, y_train, x_validation, y_validation, \n",
    "                    train_batches = 1, train_shuffle = False,\n",
    "                    validation_batches = 1, validation_shuffle = False):\n",
    "    \n",
    "        # Define your DataLoader for the training set\n",
    "        train_batch_size = len(x_train) // train_batches\n",
    "        train_dataset = LearningDataset(x_train, y_train)    \n",
    "        train_loader = utils_data.DataLoader(\n",
    "            train_dataset, batch_size=train_batch_size, shuffle=train_shuffle\n",
    "        )\n",
    "\n",
    "        # Define your DataLoader for the validation set\n",
    "        validation_batch_size = len(x_validation) // validation_batches\n",
    "        validation_dataset = LearningDataset(x_validation, y_validation)\n",
    "        validation_loader = utils_data.DataLoader(\n",
    "            validation_dataset, batch_size=validation_batch_size, shuffle=validation_shuffle\n",
    "        )\n",
    "        return train_loader, validation_loader\n",
    "\n",
    "def train_validate(model, criterion, optimizer, train_loader, validation_loader, n_epochs=200):\n",
    "    training_loss_lst = []\n",
    "    training_accuracy_lst = []\n",
    "    validation_loss_lst = []\n",
    "    validation_accuracy_lst = []\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        sum_train_loss = 0.0\n",
    "        sum_train_accuracy = 0.0\n",
    "        sum_validation_loss = 0.0\n",
    "        sum_validation_accuracy = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        for train_batch_idx, (x_train, y_train) in enumerate(train_loader):\n",
    "            x_train = x_train.to(device)\n",
    "            y_train = y_train.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_prediction = model(x_train)\n",
    "            train_loss = criterion(train_prediction, y_train)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_train_loss += train_loss.item()\n",
    "            sum_train_accuracy += (torch.argmax(train_prediction, 1) == torch.argmax(y_train, 1)).float().mean()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for validation_batch_idx, (x_validation, y_validation) in enumerate(validation_loader):\n",
    "                x_validation = x_validation.to(device)\n",
    "                y_validation = y_validation.to(device)\n",
    "                validation_prediction = model(x_validation)\n",
    "                validation_loss = criterion(validation_prediction, y_validation)\n",
    "                sum_validation_loss += validation_loss.item()\n",
    "                #print(f'validation_predictions {validation_predictions}')\n",
    "                #print(f'argmax(validation_predictions {torch.argmax(validation_predictions, 1)}')\n",
    "                #print(f'xx {(torch.argmax(validation_predictions, 1) == torch.argmax(y_validation, 1)).float()}')\n",
    "                sum_validation_accuracy += (torch.argmax(validation_prediction, 1) == torch.argmax(y_validation, 1)).float().mean()\n",
    "                \n",
    "        \n",
    "        modulus = 1 if (n_epochs // 10) == 0 else (n_epochs // 10)\n",
    "        if epoch % modulus == 0:\n",
    "            training_loss_lst.append(sum_train_loss / (train_batch_idx+1))\n",
    "            training_accuracy_lst.append(sum_train_accuracy / (train_batch_idx+1))\n",
    "            validation_loss_lst.append(sum_validation_loss / (validation_batch_idx+1))\n",
    "            validation_accuracy_lst.append(sum_validation_accuracy / (validation_batch_idx+1))\n",
    "            #print(f'Epoch {epoch + 1}: Train Loss = {training_loss_lst[:-1]:,.5f}, Val Loss = {validation_loss_lst[:-1]:,.5f}, Val Accuracy = {validation_accuracy_lst[:-1]:,.5f}')\n",
    "            print(f'Epoch1 {epoch + 1}: Train Loss = {sum_train_loss}, Val Loss = {sum_validation_loss}, Val Accuracy = {sum_validation_accuracy}')\n",
    "            \n",
    "            \n",
    "    model = model.to(\"cpu\")    \n",
    "            \n",
    "    return training_loss_lst, training_accuracy_lst, validation_loss_lst, validation_accuracy_lst\n",
    "\n",
    "\n",
    "def cross_validation_training(x_data, y_data, model, criterion, optimizer, k=5, shuffle=False, n_epochs=200):\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "    \n",
    "    # Set up cross-validation\n",
    "    kf = KFold(n_splits=k, shuffle=shuffle)\n",
    "    # Train and evaluate the model for each fold\n",
    "    for fold, (train_indices, validation_indices) in enumerate(kf.split(x_data)):\n",
    "        print(f'{fold=}')\n",
    "        # Split the data into training and validation sets for this fold\n",
    "        x_train = x_data[train_indices]\n",
    "        y_train = y_data[train_indices]\n",
    "        x_validation = x_data[validation_indices]\n",
    "        y_validation = y_data[validation_indices]\n",
    "\n",
    "        train_loader, validation_loader = get_dataloaders(\n",
    "                    x_train, y_train, x_validation, y_validation, \n",
    "                    train_shuffle = True,\n",
    "                    validation_shuffle = True)\n",
    "        \n",
    "        scores = train_validate(model, criterion, optimizer, train_loader, validation_loader, n_epochs)\n",
    "        training_loss_lst, training_accuracy_lst, validation_loss_lst, validation_accuracy_lst = scores\n",
    "        \n",
    "        # Add the training and validation loss values to the lists\n",
    "        train_losses.append(training_loss_lst[:-1][0])\n",
    "        train_accuracies.append(training_accuracy_lst[:-1][0])\n",
    "        validation_losses.append(validation_loss_lst[:-1][0])\n",
    "        validation_accuracies.append(validation_accuracy_lst[:-1][0])\n",
    "    return train_losses, train_accuracies, validation_losses, validation_accuracies\n",
    "\n",
    "def plot_learning_curve(train_sizes, train_scores_lst, test_scores_lst):\n",
    "    plt.figure()\n",
    "    plt.title(\"title\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score (Accuracy)\")\n",
    "    \n",
    "    train_scores = np.array(train_scores_lst)\n",
    "    test_scores = np.array(test_scores_lst)\n",
    "    \n",
    "    #print(f'{type(train_scores)=} {train_scores=}')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    \n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, '^--', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    print(\"train_scores_mean {}\".format(train_scores_mean))\n",
    "    print(\"test_scores_mean {}\".format(test_scores_mean))\n",
    "    return plt, train_scores_mean, test_scores_mean\n",
    "        \n",
    "\n",
    "def plot_learning_curve2(train_loss, validation_loss):\n",
    "    # Plot the learning curve\n",
    "    plt.plot(train_loss, label='Training loss')\n",
    "    plt.plot(validation_loss, label='Validation loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def confusion_matrix_plot(fig, estimator, X, y, file_prefix,target_names):\n",
    "    index = file_prefix.rindex(\"/\")\n",
    "    title = file_prefix[index + 1:].replace(\"_\",\" \").title()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    disp = plot_confusion_matrix(estimator, X, y,\n",
    "                                 display_labels=sorted(target_names),\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 #normalize='pred',\n",
    "                                 values_format='n',\n",
    "                                 ax=ax)\n",
    "    disp.ax_.set_title(\"Confusion Matrix for\\n{}\".format(title))\n",
    "    plt.savefig('{}_cm.png'.format(file_prefix))\n",
    "    plt.clf()\n",
    "\n",
    "    return plt\n",
    "\n",
    "# Get a percentage of the data to train on\n",
    "def get_percent_of_data(x_data, y_data, percentage):\n",
    "    num_elements = int(len(x_data) * (percentage/100))\n",
    "    randomstate = np.random.default_rng( 0 )\n",
    "    element_indexes = randomstate.choice(len(x_data), num_elements, replace=False, shuffle=False )\n",
    "    return x_data[element_indexes], y_data[element_indexes]\n",
    "\n",
    "\n",
    "def create_results_df(model, x_data, y_data, one_hot_encoder):\n",
    "    total_correct=0\n",
    "\n",
    "    position_lst = []\n",
    "    predicted_lst = []\n",
    "    actual_lst = []\n",
    "    predicted_decoded_lst = []\n",
    "    actual_decoded_lst = []\n",
    "    for i in range(len(x_data)-1):\n",
    "        x_batch = x_data[i:i+1]\n",
    "        predicted = model(x_batch)\n",
    "        print(predicted.detach().numpy())\n",
    "        predicted_decoded = one_hot_encoder.inverse_transform(predicted.detach().numpy()) \n",
    "        actual_decoded = one_hot_encoder.inverse_transform(y_data[i:i+1].detach().numpy()) \n",
    "        #print(f'{X_batch} {predicted} {y_batch[i:i+1]}')\n",
    "        p_a = torch.argmax(predicted, 1)[0]\n",
    "        t_a = torch.argmax(y_data[i:i+1],1)[0]\n",
    "\n",
    "        position_lst.append(i)\n",
    "        predicted_lst.append(p_a.item())\n",
    "        actual_lst.append(t_a.item())\n",
    "\n",
    "        predicted_decoded_lst.append(predicted_decoded)\n",
    "        actual_decoded_lst.append(actual_decoded)\n",
    "        #print(f'{i:<3} {p_a} {t_a} {p_a==t_a}')\n",
    "        if p_a==t_a:\n",
    "            total_correct +=1\n",
    "    print(f'Score {total_correct/len(x_data)}')\n",
    "    results = pd.DataFrame({'position':position_lst,'predicted_cd':predicted_decoded_lst,'actual_cd':actual_decoded_lst,'predicted':predicted_lst,'actual':actual_lst})\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842f008-16a4-4ba2-bb82-186bdb49d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, one_hot_encoder = get_life_stages_data(BASE_DIR)\n",
    "x_test, y_test, _ = get_life_stages_data(BASE_DIR, data_type='test')\n",
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfedf06-0e32-47c0-adce-94a85dc19825",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = MultiClassClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_loader, validation_loader = get_dataloaders(\n",
    "                    x_train, y_train, x_test, y_test, \n",
    "                    train_shuffle = True, validation_shuffle = True)\n",
    "\n",
    "for batch_idx, (x_validation, y_validation) in enumerate(validation_loader):\n",
    "    print(f'{x_validation[0]=} {len(y_validation[0])=}')\n",
    "    \n",
    "for batch_idx, (x_train, y_train) in enumerate(train_loader):\n",
    "    print(f'{x_train[0]=} {len(y_train[0])=}')\n",
    "\n",
    "start = time.time()    \n",
    "train_losses, validation_losses, validation_accuracies = train_validate(model, criterion, optimizer, train_loader, validation_loader, n_epochs=400)\n",
    "print(formatted_elapsed_time(start,end=None))\n",
    "#validation_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4589733-8498-41c7-8301-d6f16cdc6fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plot_learning_curve2(train_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db063ec-29b0-43ee-bc25-cf06fd0e6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_one_hot(arr):\n",
    "    # Find the index of the maximum value in the array\n",
    "    max_index = np.argmax(arr)\n",
    "    # Create a new array of zeros with the same shape as the input array\n",
    "    one_hot = np.zeros_like(arr)\n",
    "    # Set the index of the maximum value to 1\n",
    "    one_hot[max_index] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_pred = model(x_test)\n",
    "y_pred = y_pred.detach().numpy()\n",
    "y_pred = np.apply_along_axis(get_one_hot, axis=1, arr=y_pred)\n",
    "y_pred\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(\"F1 score: {:.2f}\".format(f1))\n",
    "\n",
    "#results = create_results_df(model,x_test,y_test, one_hot_encoder)\n",
    "#results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3a25e-1188-4d29-800d-8ec9c64d7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results against the training set\n",
    "results = create_results_df(model,x_train,y_train, one_hot_encoder)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b78574b-77a5-4555-b452-81a90117b9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Run cross-valivation once with all the data\n",
    "\n",
    "model = MultiClassClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "train_losses, validation_losses, validation_accuracies = cross_validation_training(x_train, y_train, model, criterion, optimizer, k=5,shuffle=False, n_epochs=100)\n",
    "\n",
    "plot_learning_curve2(train_losses, validation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6060a-40ae-4c89-a63c-9042bc6b6fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Run cross-valivation once with all the data\n",
    "\n",
    "x_train, y_train, one_hot_encoder = get_life_stages_data(BASE_DIR)\n",
    "x_test, y_test, one_hot_encoder = get_life_stages_data(BASE_DIR, data_type='test')\n",
    "\n",
    "model = MultiClassClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses_lst =[]\n",
    "validation_losses_lst =[]\n",
    "validation_accuracies_lst =[]\n",
    "for percentage in [20,40,60,80,100]:\n",
    "    print(f\"Training with {percentage}% of data\")\n",
    "    x_data, y_data = get_percent_of_data(x_train, y_train, percentage)\n",
    "    train_losses, validation_losses, validation_accuracies  = cross_validation_training(x_data, y_data, model, criterion, optimizer, k=5,shuffle=False, n_epochs=300)\n",
    "    train_losses_lst.append(train_losses)\n",
    "    validation_losses_lst.append(validation_losses)\n",
    "    validation_accuracies_lst.append(validation_accuracies)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9f925-9d73-4320-bab4-1974b4f2eb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss as we add more data\n",
    "plot_learning_curve([20,40,60,80,100], train_losses_lst, validation_losses_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211ae5c-c2c3-4a83-bdcd-bd385fc1db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results against the training set\n",
    "results = create_results_df(model,x_train,y_train, one_hot_encoder)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f614393-cd6f-4d3f-a4d4-fb1e17f77bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual category descriptions (codes)\n",
    "results_actual_df = results['actual_cd'].value_counts()\n",
    "results_actual_df = results_actual_df.to_frame()\n",
    "results_actual_df = results_actual_df.reset_index()\n",
    "results_actual_df = results_actual_df.rename(columns={'index':'category_1','actual_cd':'actual'})\n",
    "results_actual_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102300b2-28a4-4229-b525-03210130838c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted category descriptions (codes)\n",
    "results_predicted_df = results['predicted_cd'].value_counts()\n",
    "results_predicted_df = results_predicted_df.to_frame()\n",
    "results_predicted_df = results_predicted_df.reset_index()\n",
    "results_predicted_df = results_predicted_df.rename(columns={'index':'category_1','predicted_cd':'predicted'})\n",
    "results_predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232bcf5-a82e-4e60-b5ee-9cf4d456359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac5f673-ea86-4233-9c72-d5ceca25f07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_correct = results.query(\"predicted==actual\")\n",
    "predicted_correct_df = predicted_correct['predicted_cd'].value_counts()\n",
    "predicted_correct_df = predicted_correct_df.to_frame()\n",
    "predicted_correct_df = predicted_correct_df.reset_index()\n",
    "predicted_correct_df = predicted_correct_df.rename(columns={'index':'category_1','predicted_cd':'correct'})\n",
    "predicted_correct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a3bf0-5a9e-4604-abfb-afd52c7d22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_incorrect = results.query(\"predicted!=actual\")\n",
    "predicted_incorrect_df = predicted_incorrect['predicted_cd'].value_counts()\n",
    "predicted_incorrect_df = predicted_incorrect_df.to_frame()\n",
    "predicted_incorrect_df = predicted_incorrect_df.reset_index()\n",
    "predicted_incorrect_df = predicted_incorrect_df.rename(columns={'index':'category_1','predicted_cd':'incorrect'})\n",
    "predicted_incorrect_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c74421-67c4-4bf7-803c-ad7ed962a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_actual_df['category_1'] = results_actual_df['category_1'].astype(str)\n",
    "results_predicted_df['category_1'] = results_predicted_df['category_1'].astype(str)\n",
    "results_df = pd.merge(results_actual_df, results_predicted_df, on='category_1', how='left')\n",
    "predicted_correct_df['category_1'] = predicted_correct_df['category_1'].astype(str)\n",
    "results_df = pd.merge(results_df, predicted_correct_df, on='category_1', how='left')\n",
    "predicted_incorrect_df['category_1'] = predicted_incorrect_df['category_1'].astype(str)\n",
    "results_df = pd.merge(results_df, predicted_incorrect_df, on='category_1', how='left')\n",
    "results_df.to_csv(f'{out_dir}/results.csv', index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf775226-e488-4dc3-b216-a324f5947090",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = create_results_df(model,x_test, y_test, one_hot_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a9f03-063e-4878-b869-42ec02076a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = results.query(\"predicted!=actual\")\n",
    "correct['predicted_cd'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126b64a-0a68-420a-a7c8-1b784f9f8334",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = results.query(\"predicted==actual\")\n",
    "correct['predicted_cd'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfe814-3e75-44a4-8a5d-1a9adabcafe8",
   "metadata": {},
   "source": [
    "## Visualizing a PyTorch Model\n",
    "\n",
    "\n",
    "* Save your PyTorch model in an exchange format\n",
    "* Use Netron to create a graphical representation.\n",
    "\n",
    "https://netron.app/\n",
    "\n",
    "https://machinelearningmastery.com/visualizing-a-pytorch-model/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5f7e30-71bc-4796-ae85-f5377edc8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, x_test, './output_data/multi_class_classifier.onnx', input_names=[\"features\"], output_names=[\"logits\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3ab0e-8d26-4ac2-bc0d-df8967280b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dan-dev-sc]",
   "language": "python",
   "name": "conda-env-dan-dev-sc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
